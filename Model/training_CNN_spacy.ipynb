{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()+'/'\n",
    "dataset_path =  current_path+'Collected Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function reads the csv files saved in the directory\n",
    "df_train = pd.read_csv(dataset_path+'train.csv') \n",
    "df_dev = pd.read_csv(dataset_path+'devel.csv')\n",
    "df_test = pd.read_csv(dataset_path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function combines all 3 datasets\n",
    "df_all = df_train.append(df_test.append(df_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function removes all the punctuation and takes care of all the word\n",
    "pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function clean all the symbols and removes them from the text\n",
    "df_all['clean_message'] = df_all['message'].str.findall(pattern).str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_message</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>DeionSandersJr DeionSanders bad Slash prices s...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>November canola lost 464 per tonne</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Iniesta still the one player idolize from Barc...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_message  emotion\n",
       "1681  DeionSandersJr DeionSanders bad Slash prices s...     fear\n",
       "2406                 November canola lost 464 per tonne  sadness\n",
       "149   Iniesta still the one player idolize from Barc...      joy"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this displays the message with the emotion\n",
    "df_all = df_all[['clean_message', 'emotion']]\n",
    "df_all.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How the Who the heck moved fridge should knock the landlord door angry mad',\n",
       "  'anger'),\n",
       " ('Indian Uber driver just called someone the word wasn moving vehicle have jumped out disgusted',\n",
       "  'anger'),\n",
       " ('DPD asked for parcel delivered pick store not address fuming poorcustomerservice',\n",
       "  'anger'),\n",
       " ('whichever butt wipe pulled the fire alarm davis was sound asleep pissed angry upset tired sad tired hangry',\n",
       "  'anger'),\n",
       " ('Don join BTCare they put the phone down you talk over you and are rude Taking money out acc willynilly fuming',\n",
       "  'anger'),\n",
       " ('blood boiling', 'anger'),\n",
       " ('When you still got whole season Wentworth watch and stupid cunt work ruins for KirstyGA raging oldcunt',\n",
       "  'anger'),\n",
       " ('why does tracking show equipment delivered when wasn Why service suddenly delayed already weeks fuming',\n",
       "  'anger'),\n",
       " ('TeamShanny legit why furious with him people are such fucking idiots',\n",
       "  'anger'),\n",
       " ('How suppose work you that Wtf dude Thanks for pissing off', 'anger')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function takes the message and emotion as a tuple i.e. ()\n",
    "df_all['tuples'] = df_all.apply(lambda row: (row['clean_message'],row['emotion']), axis=1)\n",
    "train =df_all['tuples'].tolist()\n",
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en - (this downloads spacy framework) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loads the spacy model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'textcat']\n"
     ]
    }
   ],
   "source": [
    "#the function textcat helps to create a category to the text which contains no category\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "# Adding the built-in textcat component to the pipeline.\n",
    "    textcat=nlp.create_pipe( \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    print(nlp.pipe_names)\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function gets all the unique labels from the dataset\n",
    "uniq_labels = list(df_all['emotion'].unique().tolist())\n",
    "\n",
    "#this makes the categories in the spacy format \n",
    "def get_categories(categories, uniq_labels_):\n",
    "    cats = dict(zip(uniq_labels_,[0.0]*len(uniq_labels_)))\n",
    "    for category in categories:\n",
    "        cats[category] = 1.0\n",
    "    return cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_label in uniq_labels:\n",
    "    textcat.add_label(each_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There always that one song which makes you turn the radio you rather sit silence awful',\n",
       "  {'cats': {'anger': 0.0,\n",
       "    'normal': 0.0,\n",
       "    'fear': 1.0,\n",
       "    'sadness': 0.0,\n",
       "    'joy': 0.0}})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function enable to split the dataset into train and development in a random way\n",
    "import random\n",
    "\n",
    "def load_data(limit=0, split=0.8):\n",
    "    train_data=train\n",
    "    # Shuffle the data\n",
    "    random.shuffle(train_data)\n",
    "    texts, labels = zip(*train_data)\n",
    "    # get the categories for each review\n",
    "    cats = [get_categories([y],uniq_labels) for y in labels]\n",
    "\n",
    "    # Splitting the training and evaluation data\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "n_texts=7000\n",
    "\n",
    "# Calling the load_data() function \n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "\n",
    "# Processing the final format of training data\n",
    "train_data = list(zip(train_texts,[{'cats': cats} for cats in train_cats]))\n",
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this trains the model and evaluates based on the dataset\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
    "\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "1.624\t0.783\t0.765\t0.774\n",
      "1.426\t0.786\t0.767\t0.776\n",
      "1.284\t0.784\t0.773\t0.778\n",
      "1.503\t0.787\t0.774\t0.780\n",
      "1.425\t0.782\t0.773\t0.778\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Disabling other components\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this funtion round's off the floating point number into 2 decimal places\n",
    "def roundoff(dict_y):\n",
    "    for k, v in dict_y.items():\n",
    "        v = round(v,2) \n",
    "        dict_y[k] = v \n",
    "    return dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0.0, 'normal': 0.0, 'fear': 0.09, 'sadness': 0.0, 'joy': 0.91}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function tets the emotion of the inputted text\n",
    "test_text= \"i love you\"\n",
    "doc=nlp(test_text)\n",
    "roundoff(doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /Users/vineeth/Dropbox/Code Base\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = current_path\n",
    "new_model_name = 'emotion_detection'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.meta['name'] = new_model_name  # rename model\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: Flask in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (1.1.2)\r\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (from Flask) (7.1.2)\r\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (from Flask) (2.11.2)\r\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (from Flask) (1.0.1)\r\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (from Flask) (1.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /Users/vineeth/opt/miniconda3/lib/python3.8/site-packages (from Jinja2>=2.10.1->Flask) (1.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flasks(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello, World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vineeth/Dropbox/Code Base/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
